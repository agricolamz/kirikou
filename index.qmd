---
title: "Kirikou NLP"
date: today
date-format: D.MM.YYYY
format: html
df-print: paged
execute:
  warning: false
  message: false
  fig-width: 9
editor: source
code-fold: true
editor_options: 
  chunk_output_type: console
---

Я решил разбить все на три группы по переменным `socio_family_adoptive` и `residence_place`. Вот самые частотные униграммы:

```{r}
#| message: false

# setwd("/home/agricolamz/work/articles/2025_kirikou/repo/")
library(tidyverse)
theme_set(theme_minimal()+theme(text = element_text(size = 16)))
library(tidytext)
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  mutate(txenfant = str_remove_all(txenfant , "[/+]"),
         txenfant = str_remove_all(txenfant, "\\[.*?\\]")) |>
  filter(!is.na(txenfant)) |> 
  group_by(name, age, socio_family_adoptive, text_type, residence_place) |> 
  summarize(text = str_c(txenfant, collapse = " ")) |> 
  ungroup() |> 
  mutate(soc = str_c(socio_family_adoptive, " (", residence_place, ")"),
         soc = factor(soc, levels = c("Mano (Gody)", "Mano (Nzerekore)", "Bilingual (Nzerekore)")),
         age_group = "children") ->
  corpus

readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |> 
  mutate(age = 100) |> 
  rename(txenfant = `txenfant = tx`) |> 
  filter(!is.na(txenfant)) |> 
  mutate(txenfant = str_remove_all(txenfant , "[/+]"),
         txenfant = str_remove_all(txenfant, "\\[.*?\\]")) |>
  group_by(name, age, socio_family_adoptive, residence_place) |> 
  summarize(text = str_c(txenfant, collapse = " ")) |> 
  ungroup() |> 
  mutate(soc = str_c(socio_family_adoptive, " (", residence_place, ")"),
         soc = factor(soc, levels = c("Mano (Bossou)", "Mano (Nzerekore)", "Bilingual (Le Teil)")),
         age_group = "adults",
         text_type = "folktales") ->
  corpus_ad

corpus |> 
  unnest_tokens(input = text, output = word) |> 
  count(soc, word) |> 
  group_by(soc) |> 
  slice_max(n, n = 20) |> 
  mutate(word = reorder_within(word, by = n, within = soc)) |>  
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~soc, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

Из странного я вижу два разных gbaa, которые видимо, по разному набраны. Посмотрим на все символы, которые используются в тексте:

```{r}
corpus |> 
  unnest_tokens(input = text, output = characters, token = "characters") |> 
  distinct(characters) |> 
  pull(characters) |> 
  sort()
```

Видимо, проблема с символами, которые записаны по-французски и в МФА, а также с некоторыми символами: исправим.

```{r}
corpus |> 
  mutate(text = str_replace_all(text, "á", "á"),
         text = str_replace_all(text, "á", "á"),
         text = str_replace_all(text, "à", "à"),
         text = str_replace_all(text, "à", "à"),
         text = str_replace_all(text, "á̰", "á̰"),
         text = str_replace_all(text, "á̰", "á̰"),
         text = str_replace_all(text, "à̰", "à̰"),
         text = str_replace_all(text, "à̰", "à̰"),         
         text = str_replace_all(text, "ā", "ā"),
         text = str_replace_all(text, "ā", "ā"),
         text = str_replace_all(text, "ā̰", "ā̰"),
         text = str_replace_all(text, "ā̰", "á̰"),
         text = str_replace_all(text, "a̰", "a̰"),
         text = str_replace_all(text, "é", "é"),
         text = str_replace_all(text, "é", "é"),
         text = str_replace_all(text, "è", "è"),
         text = str_replace_all(text, "è", "è"),
         text = str_replace_all(text, "è", "è"),         
         text = str_replace_all(text, "ē", "ē"),
         text = str_replace_all(text, "ē", "ē"),   
         text = str_replace_all(text, "ɛ̄", "ɛ̄"), 
         text = str_replace_all(text, "ɛ̰̀", "ɛ̰̀"),
         text = str_replace_all(text, "ɛ̰̀", "ɛ̰̀"),
         text = str_replace_all(text, "ɛ̰́", "ɛ̰́"),
         text = str_replace_all(text, "ɛ̰́", "ɛ̰́"),
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),  
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),         
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),         
         text = str_replace_all(text, "í", "í"),
         text = str_replace_all(text, "í", "í"),
         text = str_replace_all(text, "ì", "ì"),
         text = str_replace_all(text, "ì", "ì"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ́", "ḭ́"),
         text = str_replace_all(text, "ḭ̀", "ḭ̀"),
         text = str_replace_all(text, "ḭ̀", "ḭ̀"),
         text = str_replace_all(text, "ɩ̀", "ì"),
         text = str_replace_all(text, "ó", "ó"),
         text = str_replace_all(text, "ó", "ó"),
         text = str_replace_all(text, "ò", "ò"),
         text = str_replace_all(text, "ò", "ò"),
         text = str_replace_all(text, "ō", "ō"),
         text = str_replace_all(text, "ō", "ō"),
         text = str_replace_all(text, "ɔ̰́", "ɔ̰́"),
         text = str_replace_all(text, "ɔ̰́", "ɔ̰́"),
         text = str_replace_all(text, "ɔ̰̀", "ɔ̰̀"),
         text = str_replace_all(text, "ɔ̰̀", "ɔ̰̀"),
         text = str_replace_all(text, "ɔ̰̄", "ɔ̰̄"),
         text = str_replace_all(text, "ɔ̰̄", "ɔ̰̄"),
         text = str_replace_all(text, "ú", "ú"),
         text = str_replace_all(text, "ú", "ú"),
         text = str_replace_all(text, "ù", "ù"),
         text = str_replace_all(text, "ù", "ù"),
         text = str_replace_all(text, "ū", "ū"),
         text = str_replace_all(text, "ū", "ū"),
         text = str_replace_all(text, "ṵ́", "ṵ́"),
         text = str_replace_all(text, "ṵ́", "ṵ́"),
         text = str_replace_all(text, "ṵ̄", "ṵ̄"),
         text = str_replace_all(text, "ṵ̄", "ṵ̄"),
         text = str_remove_all(text, "[123¹]")) ->
  corpus

corpus_ad |> 
  mutate(text = str_replace_all(text, "á", "á"),
         text = str_replace_all(text, "á", "á"),
         text = str_replace_all(text, "à", "à"),
         text = str_replace_all(text, "à", "à"),
         text = str_replace_all(text, "á̰", "á̰"),
         text = str_replace_all(text, "á̰", "á̰"),
         text = str_replace_all(text, "à̰", "à̰"),
         text = str_replace_all(text, "à̰", "à̰"),         
         text = str_replace_all(text, "ā", "ā"),
         text = str_replace_all(text, "ā", "ā"),
         text = str_replace_all(text, "ā̰", "ā̰"),
         text = str_replace_all(text, "ā̰", "á̰"),
         text = str_replace_all(text, "a̰", "a̰"),
         text = str_replace_all(text, "é", "é"),
         text = str_replace_all(text, "é", "é"),
         text = str_replace_all(text, "è", "è"),
         text = str_replace_all(text, "è", "è"),
         text = str_replace_all(text, "è", "è"),
         text = str_replace_all(text, "ē", "ē"),
         text = str_replace_all(text, "ē", "ē"),   
         text = str_replace_all(text, "ɛ̄", "ɛ̄"), 
         text = str_replace_all(text, "ɛ̰̀", "ɛ̰̀"),
         text = str_replace_all(text, "ɛ̰̀", "ɛ̰̀"),
         text = str_replace_all(text, "ɛ̰́", "ɛ̰́"),
         text = str_replace_all(text, "ɛ̰́", "ɛ̰́"),
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),  
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),         
         text = str_replace_all(text, "ɛ̰̄", "ɛ̰̄"),         
         text = str_replace_all(text, "í", "í"),
         text = str_replace_all(text, "í", "í"),
         text = str_replace_all(text, "ì", "ì"),
         text = str_replace_all(text, "ì", "ì"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ī", "ī"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ̄", "ḭ̄"),
         text = str_replace_all(text, "ḭ́", "ḭ́"),
         text = str_replace_all(text, "ḭ̀", "ḭ̀"),
         text = str_replace_all(text, "ḭ̀", "ḭ̀"),
         text = str_replace_all(text, "ɩ̀", "ì"),
         text = str_replace_all(text, "ó", "ó"),
         text = str_replace_all(text, "ó", "ó"),
         text = str_replace_all(text, "ò", "ò"),
         text = str_replace_all(text, "ò", "ò"),
         text = str_replace_all(text, "ō", "ō"),
         text = str_replace_all(text, "ō", "ō"),
         text = str_replace_all(text, "ɔ̰́", "ɔ̰́"),
         text = str_replace_all(text, "ɔ̰́", "ɔ̰́"),
         text = str_replace_all(text, "ɔ̰̀", "ɔ̰̀"),
         text = str_replace_all(text, "ɔ̰̀", "ɔ̰̀"),
         text = str_replace_all(text, "ɔ̰̄", "ɔ̰̄"),
         text = str_replace_all(text, "ɔ̰̄", "ɔ̰̄"),
         text = str_replace_all(text, "ú", "ú"),
         text = str_replace_all(text, "ú", "ú"),
         text = str_replace_all(text, "ù", "ù"),
         text = str_replace_all(text, "ù", "ù"),
         text = str_replace_all(text, "ū", "ū"),
         text = str_replace_all(text, "ū", "ū"),
         text = str_replace_all(text, "ṵ́", "ṵ́"),
         text = str_replace_all(text, "ṵ́", "ṵ́"),
         text = str_replace_all(text, "ṵ̄", "ṵ̄"),
         text = str_replace_all(text, "ṵ̄", "ṵ̄"),
         text = str_remove_all(text, "[123¹]")) ->
  corpus_ad
```

Вот что получается после преобразования:

```{r}
corpus |> 
  unnest_tokens(input = text, output = characters, token = "characters") |> 
  distinct(characters) |> 
  pull(characters) |> 
  sort()
```


### Частотность

Теперь мы можем снова нарисовать наш график:

```{r}
corpus |> 
  unnest_tokens(input = text, output = word) |> 
  count(soc, word) |> 
  group_by(soc) |> 
  slice_max(n, n = 20) |> 
  mutate(word = reorder_within(word, by = n, within = soc)) |> 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~soc, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

Посмотрим на биграммы

```{r}
corpus |> 
  unnest_tokens(input = text, output = ngram, token = "ngrams", n = 2) |> 
  count(soc, ngram) |> 
  group_by(soc) |> 
  slice_max(n, n = 20) |> 
  mutate(ngram = reorder_within(ngram, by = n, within = soc)) |> 
  ggplot(aes(n, ngram))+
  geom_col()+
  facet_wrap(~soc, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

Посмотрим на трииграммы

```{r}
corpus |> 
  unnest_tokens(input = text, output = ngram, token = "ngrams", n = 3) |> 
  count(soc, ngram) |> 
  group_by(soc) |> 
  slice_max(n, n = 20) |> 
  mutate(ngram = reorder_within(ngram, by = n, within = soc)) |> 
  ggplot(aes(n, ngram))+
  geom_col()+
  facet_wrap(~soc, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

### tf-idf

Посчитаем меру tf-idf для униграм, биграм и триграм

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |>
  count(soc, word, sort = TRUE) |> 
  bind_tf_idf(word, soc, n) |> 
  arrange(desc(tf_idf)) |> 
  group_by(soc) |> 
  slice_max(order_by = tf_idf, n = 10) |> 
  mutate(word = reorder_within(word, n, soc)) |> 
  ggplot(aes(word, n)) +
  geom_bar(stat="identity") +
  facet_wrap(~ soc, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = NULL)

corpus |> 
  unnest_tokens(input = "text", output = "ngram", token = "ngrams", n = 2) |>
  count(soc, ngram, sort = TRUE) |> 
  bind_tf_idf(ngram, soc, n) |> 
  arrange(desc(tf_idf)) |> 
  group_by(soc) |> 
  slice_max(order_by = tf_idf, n = 10) |> 
  mutate(ngram = reorder_within(ngram, n, soc)) |> 
  ggplot(aes(ngram, n)) +
  geom_bar(stat="identity") +
  facet_wrap(~ soc, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = NULL)

corpus |> 
  unnest_tokens(input = "text", output = "ngram", token = "ngrams", n = 3) |>
  count(soc, ngram, sort = TRUE) |> 
  bind_tf_idf(ngram, soc, n) |> 
  arrange(desc(tf_idf)) |> 
  group_by(soc) |> 
  slice_max(order_by = tf_idf, n = 10) |> 
  mutate(ngram = reorder_within(ngram, n, soc)) |> 
  ggplot(aes(ngram, n)) +
  geom_bar(stat="identity") +
  facet_wrap(~ soc, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = NULL)
```

### Lexical diversity

> Но я подумала что было бы интересно не самое частотное посчитать, а наоборот Предположительно оно должно различаться между группами

> The American psychologist and speech pathologist Wendell Johnson (1939, 1944) proposed the type-token ratio (TTR)  (Javris 2019)

Доля уникальных слов из всех слов. Размер --- количество слов в тексте. 

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |> 
  count(name, age, soc, word) |> 
  group_by(name, age, soc) |> 
  summarise(size = sum(n),
            richness = n(),
            TTR = richness/size) |> 
  ggplot(aes(age, TTR, color = soc, label = name))+
  geom_point(aes(size = size))+
  ggrepel::geom_text_repel()+
  labs(size = NULL, color = NULL)
```

Энтропия текстов (чуть больше верю этой мере разнообразия).

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |> 
  count(name, age, soc, word) |> 
  group_by(name, age, soc) |> 
  summarise(size = sum(n),
            richness = n(),
            TTR = richness/size,
            entropy = -sum(TTR*log2(TTR))) |> 
  ggplot(aes(age, entropy, color = soc, label = name))+
  geom_point(aes(size = size))+
  ggrepel::geom_text_repel()+
  labs(size = NULL, color = NULL)
```

Оказывается (Baayen 2008, 222–36) исследовал меры разнообразия и написал несколько интересных мыслей.

```{r}
corpus |> 
  bind_rows(corpus_ad) |> 
  unnest_tokens(input = "text", output = "word", token = "words") |> 
  group_by(name, age, socio_family_adoptive, residence_place, soc) |> 
  mutate(id = 1:n(),
         word_id = as.factor(word) |> fct_inorder() |>  as.double(),
         max = cummax(word_id),
         coord_max = max(max)) |> 
  ungroup() ->
  for_visualization

for_visualization |>
  mutate(name = str_replace(name, "_", " "),
         name = str_c(name, " (", age, ")"),
         name = str_replace(name, "\\(35\\)", "\\(adult\\)")) |> 
  group_by(name, age, socio_family_adoptive, residence_place, soc, coord_max) |> 
  summarise(id = max(id)) ->
  labels

for_visualization |> 
  mutate(socio_family_adoptive = factor(socio_family_adoptive, levels = c("Mano", "Bilingual"))) |> 
  ggplot(aes(id, max, group = name))+
  geom_line(data = for_visualization |> select(-soc), color = "grey80")+
  geom_line(aes(color = residence_place))+
  geom_point(aes(id, coord_max), data = labels)+
  ggrepel::geom_text_repel(aes(id, coord_max, label = name), data = labels)+
  facet_wrap(~socio_family_adoptive)+
  theme(legend.position = "bottom")+
  labs(x = "tokens",
       y = "types")
```


### Кластеризация людей

Я взял частотные 50 слов и кластеризовал людей на основе нормализованной частотности слов в текстах людей:

```{r}
corpus |> 
  bind_rows(corpus_ad) |> 
  unnest_tokens(input = "text", output = "word", token = "words") |> 
  count(word, sort = TRUE) |>
  filter(!(word %in% c("kirikou", "karaba", "làà", "mɔ̀ɔ̀", "dàā", "yíí", "zíé", "wìì", "lòkóò"))) |> 
  slice_max(order_by = n, n = 50) |> 
  pull(word) ->
  freq_words

library(dendextend)
par(mar=c(2,0,0,18))

corpus |> 
  bind_rows(corpus_ad) |> 
  mutate(name = str_c(name, " (", age, "): ", soc),
         soc = str_extract(soc, "\\w{1,}"),
         color = c("tomato", "darkgreen")[factor(soc)]) |> 
  distinct(name, color) ->
  colors

library(ape)

corpus |> 
  bind_rows(corpus_ad) |> 
  mutate(name = str_c(name, " (", age, "): ", soc)) |> 
  unnest_tokens(input = text, output = word, token = "words") |> 
  count(name, word)  |> 
  group_by(name) |> 
  mutate(total = sum(n),
            ratio = n/total,
            ratio_normalized = scale(ratio)[1]) |> 
  filter(word %in% freq_words) |> 
  select(name, word, ratio_normalized) |> 
  pivot_wider(names_from = word, values_from = ratio_normalized, values_fill = 0) |> 
  column_to_rownames("name") |>  
  dist(method = "manhattan") |> 
  hclust(method = "complete") |> 
  as.phylo() %>%
  plot(tip.color = colors$color[match(.$tip.label, colors$name)],
       font = 2) 
```

Я попробовал разные количества слов (20-30-40-50, больше тупо брать --- совсем много пропусков буедт), но красные билингвы из Нзерекоре устойчивы, а синие-зеленые манцы перемешиваются, что в принципе подтверждает, что билингвы немного другие.

### Векторизация

Я использую простенький word2vec и umap для уменьшения размерности:

```{r}
#| fig-height: 9
#| fig-width: 12

library(word2vec)

set.seed(42) 
model <- word2vec(x = corpus$text, 
                  type = "skip-gram",
                  dim = 50,
                  window = 5,
                  iter = 20,
                  hs = TRUE,
                  min_count = 5,
                  threads = 6)
emb <- as.matrix(model)

library(uwot)
set.seed(42)
viz <- umap(emb,  n_neighbors = 15, n_threads = 2)

tibble(word = rownames(emb), 
       V1 = viz[, 1], 
       V2 = viz[, 2]) |> 
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 5, alpha = 0.4)
```

Слова находящиеся рядом должны иметь общую семантику. Эта картинка каждый раз перерисовывается с рандомизацией.

### Анализ коллокаций

Две меры коллокационности, посмотри верхушку списков:

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "ngram", token = "ngrams", n = 2) |> 
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  count(word1, word2) |> 
  mutate(N = sum(n)) |> 
  group_by(word1) |> 
  mutate(R1 = sum(n),
         R2 = N - R1) |> 
  group_by(word2) |> 
  mutate(C1 = sum(n),
         C2 = N - C1) |> 
  arrange(-R1, -C1) |> 
  ungroup() |> 
  rename(O11 = n) |> 
  mutate(O12 = R1-O11,
         O21 = C1-O11,
         O22 = C2-O12, # or R2-O21 
         E11 = R1 * C1 / N, 
         E12 = R1 * C2 / N,
         E21 = R2 * C1 / N, 
         E22 = R2 * C2 / N,
         MI = log2(O11 / E11),
         liddle = (O11*O22-O12*O21)/(C1*C2), # Liddell (1976)
         dice = 2*O11/(R1+C1), # Smadja et al. (1996)
         jaccard = O11/(O11+O12+O21),
         t.score = (O11 - E11) / sqrt(O11),
         X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22,
         DP = O11 / R1 - O21 / R2) ->
  collacations

collacations |> 
  arrange(desc(t.score)) |> 
  select(word1, word2, O11, N, t.score) |> 
  rename(total = N,
         co_occurrence_frequency = O11)

collacations |> 
  arrange(desc(MI)) |> 
  select(word1, word2, O11, N, MI) |> 
  rename(total = N,
         co_occurrence_frequency = O11)
```

## Для составления списка стопслов

Можно идти по этому списку, листая страницы и выписывать стопслова. Пожалуйста, копирую отсюда написание --- я вносил правки в орфографию.

```{r}
corpus |> 
  unnest_tokens(input = text, output = word) |> 
  count(word, sort = TRUE)
```

Я забыл задачу, для которой мы это делали, если мы хотим что-то узнать про тексты, то тогда нужно грамматические, а не лексические, а если мы хотим по ним кластеризовать людей, то тогда лексические.

## Количество фр. заимствований

Я буду считать, что ниже представлен список французского. Если я не прав, то скажи, я исключу что-то из списка.

```{r}
'á" "à" "ā" "a̰" "á̰" "à̰' |> 
  str_remove_all('["a ]') |> 
  str_c("ɓɛɲŋɔ]") ->
  non_french

non_french <- str_c("[", non_french)

corpus |> 
  unnest_tokens(input = text, output = word) |> 
  count(word, sort = TRUE) |> 
  filter(str_detect(word, non_french, negate = TRUE))

corpus |> 
  unnest_tokens(input = text, output = word) |> 
  count(word, sort = TRUE) |> 
  filter(str_detect(word, non_french, negate = TRUE)) |> 
  pull(word) ->
  french_words

french_words <- french_words[!(french_words %in% c("unp", "kirikou", "kiriki", "sociere", "sorc", "kiri", "kris", "sorciere", "singing", "onomat", "a", "eee", "__", "g", "hes", "kirikou", "karaba", "keelee", "mbe", "hesitation", "pascale", "s", "ts", "zi", "z", "e", "es", "we", "he̋"))] 

french_words
```

Значение по людям:

```{r}
corpus |> 
  unnest_tokens(input = text, output = word) |> 
  mutate(french = if_else(word %in% french_words, "french", "non_french"),
         name = str_c(name, " (", age, ")")) |> 
  count(name, soc, french) |> 
  pivot_wider(names_from = french, values_from = n, values_fill = 0) 

corpus |> 
  bind_rows(corpus_ad) |> 
  unnest_tokens(input = text, output = word) |> 
  mutate(french = if_else(word %in% french_words, "french", "non_french"),
         name = str_c(name, " (", age, ")"),
         soc = str_extract(soc, "\\w{1,}")) |> 
  count(name, age_group, text_type, residence_place, soc, french) |>
  mutate(residence_place_nz = if_else(residence_place == "Nzerekore", "Nzerekore", "other"),
         variable = str_c(text_type, " (", residence_place_nz, ")")) |> 
  pivot_wider(names_from = french, values_from = n, values_fill = 0) |> 
  mutate(french = if_else(french == 0, 0.5, french),
         non_french = if_else(non_french == 0, 0.5, non_french)) |>  
  ggplot(aes(non_french, french, color = variable))+
  scale_x_log10()+
  scale_y_log10()+
  geom_smooth(method = "lm", se = FALSE, color = "grey60", linetype = 2)+
  geom_point()+
  ggrepel::geom_text_repel(aes(label = name))+
  coord_fixed()+ 
  labs(x = "non french lexicon (log scaled)",
       y = "french lexicon (log scaled)")
```

## Количество ономатопей

```{r}
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |>
  filter(!is.na(ft)) |> 
  mutate(onomatopoeia = str_extract_all(ft, "ONOMAT")) |>
  select(name, age, text_type, onomatopoeia) |> 
  unnest_longer(onomatopoeia) |> 
  na.omit() |> 
  count(name, age, text_type)
```

Что-то они всего у шесть людей...

## Diversity of tense marking

```{r}
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |> 
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  group_by(name, age, text_type) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(chunk_structure_old = chunk_structure, 
         chunk_structure = str_remove_all(chunk_structure, "(?<=\\]).*?(?=\\[)"), 
         chunk_structure = str_remove_all(chunk_structure, "<.*?>"),
         chunk_structure = str_remove_all(chunk_structure, "(NP)|(CR)|(SR)|(FRENCH)|(ONOMAT)|(SG)|(SC)|(NMLZ)|(\\.\\.\\.)"),
         chunk_structure = str_remove_all(chunk_structure, "[^A-Z_\\.\\]\\[]"),
         #chunk_structure = str_remove_all(chunk_structure, not_french),
         chunk_structure = str_replace_all(chunk_structure, "IPFVPP", "IPFV"),
         chunk_structure = str_replace_all(chunk_structure, "PRFPP", "PRF"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPP", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTADV", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "JNTPP", "JNT"),
         chunk_structure = str_replace_all(chunk_structure, "JNTADV", "JNT"),
         chunk_structure = str_replace_all(chunk_structure, "JNTPP", "JNT"),
         chunk_structure = str_replace_all(chunk_structure, "EXI\\.\\.PP", "EXI"),
         chunk_structure = str_replace_all(chunk_structure, "EXIPP", "EXI"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPP\\.", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "CONJ\\.", "CONJ"),
         chunk_structure = str_replace_all(chunk_structure, "EXI\\.DUR", "EXI"),
         chunk_structure = str_replace_all(chunk_structure, "EXIDUR", "EXI"),
         chunk_structure = str_replace_all(chunk_structure, "PST\\.DURADV", "PST"),
          chunk_structure = str_replace_all(chunk_structure, "PSTPP..ADV", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPP.PP", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "CONJPP", "CONJ"),
         chunk_structure = str_replace_all(chunk_structure, "CONJPP", "CONJ"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPST", "PSTPST"),
         chunk_structure = str_replace_all(chunk_structure, "CONJADV", "CONJ"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPHASPP", "PSTP"),
         chunk_structure = str_replace_all(chunk_structure, "PST\\.DURPP\\.", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PST\\.DURPP\\.", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PST\\.", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PST\\.", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTDUR", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTADV", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTPP", "PST"),
                  chunk_structure = str_replace_all(chunk_structure, "PSTPST", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTP", "PST"),
         chunk_structure = str_replace_all(chunk_structure, "PSTSAY", "PST\\.SAY"),
         chunk_structure = str_replace_all(chunk_structure, "JNT.SAYPP", "JNT.SAY"),
         chunk_structure = str_replace_all(chunk_structure, "JNT.SAYPP", "JNT.SAY"),
         chunk_structure = str_replace_all(chunk_structure, "IMPPP", "IMP"),
         chunk_structure = str_replace_all(chunk_structure, "JNTJNT", "JNT"),
         chunk_structure = str_replace_all(chunk_structure, "PST.NEG", "NEG"),
         chunk_structure = str_replace_all(chunk_structure, "PST.SAYPP", "PST.SAY"),
         chunk_structure = str_replace_all(chunk_structure, "PRF\\.\\.", "PRF"),
         chunk_structure = str_replace_all(chunk_structure, "PRF\\.\\.", "PRF"),
         chunk_structure = str_split(chunk_structure, "\\]\\[")) |> 
  unnest_longer(chunk_structure) |> 
  mutate(chunk_structure = str_remove_all(chunk_structure, "[\\]\\}\\[\\{]"),
         chunk_structure = str_remove_all(chunk_structure, "^[_\\.]{1,}")) |> 
  mutate(chunk_structure = str_remove(chunk_structure, "\\_.*")) |> 
  filter(chunk_structure != "",
         chunk_structure != "PP",
         chunk_structure != "NP",
         chunk_structure != "R",
         chunk_structure != "SAY") ->
  tense_marking
```

Вот какие получились столбцы:

```{r}
tense_marking |> 
  distinct(chunk_structure) |> 
  pull(chunk_structure)
```

Вот финальная таблица:

```{r}
tense_marking |> 
  mutate(chunk_structure = str_remove(chunk_structure, "\\.SAY")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE) |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) 
```

График соотношения количества JNT и PST

```{r}
tense_marking |> 
  mutate(chunk_structure = str_remove(chunk_structure, "\\.SAY")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE)  |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) |> 
  mutate(PST = if_else(PST == 0, 0.9, PST),
         JNT = if_else(JNT == 0, 0.9, JNT)) |> 
  select(name, age, text_type, PST, JNT) |> 
  mutate(ratio = PST/(PST+JNT)) |> 
  arrange(-ratio)

tense_marking |> 
  mutate(chunk_structure = str_remove(chunk_structure, "\\.SAY")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE)  |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) |> 
  mutate(PST = if_else(PST == 0, 0.9, PST),
         JNT = if_else(JNT == 0, 0.9, JNT)) |> 
  select(name, age, text_type, PST, JNT) |> 
  mutate(ratio = PST/(PST+JNT),
         name = str_c(name, " (", age, ")")) |> 
  ggplot(aes(PST, JNT, color = text_type))+
  geom_smooth(method = "lm", se = FALSE, color = "grey60", linetype = 2)+
  geom_point()+
  scale_x_log10()+
  scale_y_log10()+
  ggrepel::geom_text_repel(aes(label = name))
```

JNT.SAY vs JNT без SAY

```{r}
tense_marking |> 
  filter(str_detect(chunk_structure, "JNT")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE) |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) |> 
  mutate(ratio = JNT/(JNT+JNT.SAY)) |> 
  arrange(-ratio)

tense_marking |> 
  filter(str_detect(chunk_structure, "JNT")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE) |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) |> 
  mutate(JNT = if_else(JNT == 0, 0.9, JNT),
         JNT.SAY = if_else(JNT.SAY == 0, 0.9, JNT.SAY)) |> 
  select(name, age, text_type, JNT, JNT.SAY) |> 
  mutate(name = str_c(name, " (", age, ")")) |> 
  ggplot(aes(JNT, JNT.SAY, color = text_type))+
  geom_smooth(method = "lm", se = FALSE, color = "grey60", linetype = 2)+
  geom_point()+
  scale_x_log10()+
  scale_y_log10()+
  ggrepel::geom_text_repel(aes(label = name))
```

PST + JNT vs other

```{r}
tense_marking |> 
  mutate(pst_jnt_other = if_else(str_detect(chunk_structure, "(PST)|(JNT)"), 
                                 "PST_or_JNT",
                                 "other")) |> 
  count(name, age, text_type, pst_jnt_other, sort = TRUE) |> 
  pivot_wider(names_from = pst_jnt_other, values_from = n, values_fill = 0) |> 
  mutate(ratio = PST_or_JNT/(PST_or_JNT+other)) |> 
  arrange(-ratio)

tense_marking |> 
  mutate(pst_jnt_other = if_else(str_detect(chunk_structure, "(PST)|(JNT)"), 
                                 "PST_or_JNT",
                                 "other")) |> 
  count(name, age, text_type, pst_jnt_other, sort = TRUE) |> 
  pivot_wider(names_from = pst_jnt_other, values_from = n, values_fill = 0) |> 
  mutate(PST_or_JNT = if_else(PST_or_JNT == 0, 0.9, PST_or_JNT),
         other = if_else(other == 0, 0.9, other)) |> 
  select(name, age, text_type, PST_or_JNT, other) |> 
  mutate(name = str_c(name, " (", age, ")")) |> 
  ggplot(aes(PST_or_JNT, other, color = text_type))+
  geom_smooth(method = "lm", se = FALSE, color = "grey60", linetype = 2)+
  geom_point()+
  scale_x_log10()+
  scale_y_log10()+
  ggrepel::geom_text_repel(aes(label = name))
```

## сложный синтаксис

обсчитать префиксы маленькими перед квадратными, треугольными и фигурными скобками

```{r}
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |> 
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  group_by(name, age, text_type) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(foc = str_count(chunk_structure, "foc"),
         rel = str_count(chunk_structure, "rel"),
         temp = str_count(chunk_structure, "temp"),
         cond = str_count(chunk_structure, "cond"),
         purp = str_count(chunk_structure, "purp"),
         prec = str_count(chunk_structure, "prec"),
         compar = str_count(chunk_structure, "compar"),
         mod = str_count(chunk_structure, "mod"),
         sa = str_count(chunk_structure, "sa"),
         le = str_count(chunk_structure, "le"),
         ht = str_count(chunk_structure, "ht"),
         at = str_count(chunk_structure, "at"),
         top = str_count(chunk_structure, "top"),
         voc = str_count(chunk_structure, "voc")) |> 
  select(-chunk_structure)
```

## reference tracking 

обсчитать, сколько CR vs SR в NP и в pro

```{r}
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |> 
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  group_by(name, age, text_type) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(pro_CR = str_count(chunk_structure, "pro\\(CR\\)"),
         NP_CR = str_count(chunk_structure, "NP\\(CR\\)"),
         pro_SR = str_count(chunk_structure, "pro\\(SR\\)"),
         NP_SR = str_count(chunk_structure, "NP\\(SR\\)")) |> 
  select(-chunk_structure)
```

## Сколько всего?

```{r}
readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |>
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  add_count(name, age) |> 
  rename(n_chunks = n) |> 
  group_by(name, age, text_type, n_chunks) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(n_predecations = str_count(chunk_structure, "\\[")) |> 
  select(-chunk_structure)
```

## Уменьшение размерности

```{r}
tense_marking |> 
  mutate(chunk_structure = str_remove(chunk_structure, "\\.SAY")) |> 
  count(name, age, text_type, chunk_structure, sort = TRUE) |> 
  pivot_wider(names_from = chunk_structure, values_from = n, values_fill = 0) ->
  tense

readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |> 
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  group_by(name, age, text_type) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(foc = str_count(chunk_structure, "foc"),
         rel = str_count(chunk_structure, "rel"),
         temp = str_count(chunk_structure, "temp"),
         cond = str_count(chunk_structure, "cond"),
         purp = str_count(chunk_structure, "purp"),
         prec = str_count(chunk_structure, "prec"),
         compar = str_count(chunk_structure, "compar"),
         mod = str_count(chunk_structure, "mod"),
         sa = str_count(chunk_structure, "sa"),
         le = str_count(chunk_structure, "le"),
         ht = str_count(chunk_structure, "ht"),
         at = str_count(chunk_structure, "at"),
         top = str_count(chunk_structure, "top"),
         voc = str_count(chunk_structure, "voc")) |> 
  select(-chunk_structure) ->
  syntax

readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "kirikou Mano") |>
  select(-elicitation_date) |> 
  mutate(text_type = "kirikou") |>
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano children") |> 
              mutate(text_type = "folktales")) |> 
  select(-elicitation_date) |> 
  bind_rows(readxl::read_xlsx("kirikou_annotations.xlsx", sheet = "folktales Mano adults") |>
              mutate(text_type = "folktales",
                     age = 100)) |> 
  rename(chunk_structure = `chunk structure`) |> 
  filter(!is.na(chunk_structure)) |> 
  group_by(name, age, text_type) |> 
  summarize(chunk_structure = str_c(chunk_structure, collapse = " ")) |> 
  ungroup() |> 
  mutate(pro_CR = str_count(chunk_structure, "pro\\(CR\\)"),
         NP_CR = str_count(chunk_structure, "NP\\(CR\\)"),
         pro_SR = str_count(chunk_structure, "pro\\(SR\\)"),
         NP_SR = str_count(chunk_structure, "NP\\(SR\\)")) |> 
  select(-chunk_structure) ->
  reference_tracking

corpus |> 
  bind_rows(corpus_ad) |> 
  unnest_tokens(input = text, output = word, token = "words") |> 
  count(name, age, text_type, word)  |> 
  filter(word %in% freq_words) |> 
  pivot_wider(names_from = word, values_from = n, values_fill = 0)  ->
  lexicon

syntax |> 
  full_join(tense) |> 
  full_join(lexicon) |> 
  full_join(reference_tracking) |>  
  mutate(across(-c(name, age, text_type), function(x) scale(x))) |> 
  select(-c(name, age, text_type)) |> 
  prcomp() ->
  pca

syntax |> 
  select(name, age, text_type) |> 
  bind_cols(pca$x) |> 
  ggplot(aes(PC1, PC2, color = text_type))+
  geom_point()+
  ggrepel::geom_text_repel(aes(label = str_c(name, " (", age, ")")), max.overlaps = 100)
```

